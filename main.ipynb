{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import math\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loading human observed datasets\n",
    "featureData=pd.read_csv(\"HumanObserved-Features-Data_HO.csv\")\n",
    "diffPairsHumanData=pd.read_csv(\"diffn_pairs_HO.csv\");\n",
    "samePairsHumanData=pd.read_csv(\"same_pairs_HO.csv\");\n",
    "\n",
    "#loading gsc dataset\n",
    "gscFeatureData=pd.read_csv(\"GSC-Features_GSC.csv\")\n",
    "diffPairsGSCData=pd.read_csv(\"diffn_pairs_GSC.csv\")\n",
    "samePairsGSCData=pd.read_csv(\"same_pairs_GSC.csv\")\n",
    "#number of clusters or the number of basis fucntions\n",
    "NUMBER_OF_CLUSTERS=15;\n",
    "#the method creates a dictionary\n",
    "def createDictionary(imageNumberList,listOfListOfFeatures):\n",
    "    imageFeatureMap={};\n",
    "    for i in range(0,len(imageNumberList)):\n",
    "        imageId=str(imageNumberList[i][0]);\n",
    "        #print(imageId)\n",
    "        imageFeatureMap[imageId]=listOfListOfFeatures[i];\n",
    "    return imageFeatureMap;\n",
    "#the method returns dictionary with imageId and the corresponding features\n",
    "def getFeatureDictionary(featureData, isHumanObserverd):\n",
    "    if isHumanObserverd:\n",
    "        featureData.drop(featureData.columns[0],axis=1,inplace=True)\n",
    "        \n",
    "    onlyFeatureData=featureData.iloc[:,1:]\n",
    "    onlyImageNumbers=featureData.iloc[:,0:1]\n",
    "    listOfListOfFeatures=onlyFeatureData.values.tolist()\n",
    "    imageNumberList=onlyImageNumbers.values.tolist()\n",
    "    return createDictionary(imageNumberList,listOfListOfFeatures);\n",
    "\n",
    "#fetching the imageid-feature dictionary for human data\n",
    "imageFeatureMap=getFeatureDictionary(featureData,True);\n",
    "#fetching the imageid-feature dictionary for gsc data\n",
    "imageFeatureMapGSC=getFeatureDictionary(gscFeatureData,False);\n",
    "\n",
    "diffPairsHumanSample=diffPairsHumanData.sample(n=samePairsHumanData.shape[0])\n",
    "\n",
    "#diffPairsGscSample=diffPairsGSCData.sample(n=samePairsGSCData.shape[0])\n",
    "diffPairsGscSample=diffPairsGSCData.sample(n=10000);\n",
    "samePairsGscSample=samePairsGSCData.sample(n=10000);\n",
    "#extracting image pairs and their targets from the data file\n",
    "diffPairsHumanDataList=diffPairsHumanSample.iloc[:,0:3].values.tolist()\n",
    "samePairsHumanDataList=samePairsHumanData.iloc[:,0:3].values.tolist();\n",
    "diffPairsGSCDataList=diffPairsGscSample.iloc[:,0:3].values.tolist();\n",
    "#samePairsGSCDataList=samePairsGSCData.iloc[:,0:3].values.tolist();\n",
    "samePairsGSCDataList=samePairsGscSample.iloc[:,0:3].values.tolist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be preparing data as per the scheme\n",
    "#scheme-1:concatenation of features|scheme-2:subtraction of features\n",
    "#returning 2-d array with features(as per the scheme) and the last column as the target\n",
    "def prepareDataByScheme(pairedData,imageFeatureMap,toBeConcattenated):\n",
    "    featureArrayList=[];\n",
    "    for i in range(0,len(pairedData)):\n",
    "        imageId1=pairedData[i][0];\n",
    "        imageId2=pairedData[i][1];\n",
    "        target=pairedData[i][2];\n",
    "        feature1ArrayList=imageFeatureMap[imageId1];\n",
    "        feature2ArrayList=imageFeatureMap[imageId2];\n",
    "        #if toBeConcattenated true then concatenating else subtracting\n",
    "        if toBeConcattenated:\n",
    "            modifiedFeature=feature1ArrayList + feature2ArrayList\n",
    "        else:\n",
    "            modifiedFeature=[feature1ArrayList[i] - feature2ArrayList[i] for i in range(0,len(feature1ArrayList))];\n",
    "        \n",
    "        modifiedFeatureList=modifiedFeature\n",
    "        modifiedFeatureList.append(target);\n",
    "        featureArrayList.append(modifiedFeatureList);\n",
    "    return np.array(featureArrayList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "\n",
    "def initializeModel(data):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(100, input_dim=data.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(100,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model;f\n",
    " \n",
    "    \n",
    "    \n",
    "def oprateNeuralNetwork(model,trainingFeatureData,validationFeatureData,trainingTargets,validationTargets):   \n",
    "    validation_data_split = 0.15\n",
    "    num_epochs = 25\n",
    "    model_batch_size = 128\n",
    "    tb_batch_size = 32\n",
    "    early_patience = 100\n",
    "    trainValData=np.concatenate((trainingFeatureData,validationFeatureData));\n",
    "    trainValLabels=np.concatenate((trainingTargets,validationTargets));\n",
    "    history = model.fit(trainValData\n",
    "                        , trainValLabels\n",
    "                        , validation_split=validation_data_split\n",
    "                        , epochs=num_epochs\n",
    "                        , batch_size=model_batch_size\n",
    "                                               )\n",
    "    return history;\n",
    "\n",
    "\n",
    "\n",
    "# def printKerasGraphs(history):  \n",
    "#     get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "#     df = pd.DataFrame(history.history)\n",
    "#     df.plot(subplots=True, grid=True, figsize=(10,15))\n",
    "\n",
    "def evaluateNeuralNetwork(model,testFeatureData,testTargets):\n",
    "    loss,metric=model.evaluate(testFeatureData,testTargets,batch_size=128)\n",
    "    return metric;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_PERCENT=0.8;\n",
    "VALIDATION_DATA_PERCENT=0.1\n",
    "TEST_DATA_PERCENT=0.1\n",
    "\n",
    "# this method would be merging samewriter comparison data and the different write comparison data\n",
    "def mergeData(sameWriterData,differentWriterData):\n",
    "    sameWrLen=sameWriterData.shape[0];\n",
    "    indices=np.random.randint(0,differentWriterData.shape[0],size=sameWrLen)    \n",
    "    randomData=differentWriterData[indices];\n",
    "    #randomData=differentWriterData[0:sameWriterData.shape[0],:];\n",
    "    return np.concatenate((sameWriterData,randomData));\n",
    "\n",
    "#method would be checking if any column is 0 in the merged data(2-D array), if yes then would be returning the indexes of the zero columns\n",
    "def verifyingAllZeroColumns(mergedData):\n",
    "    return np.argwhere(np.all(mergedData[..., :] == 0, axis=0));\n",
    "#dadding noise\n",
    "def modifyingMergedData(mergedData,zeroValueColumns):\n",
    "    for i in range(0,mergedData.shape[1]):\n",
    "        mergedData[i][i]=mergedData[i][i]+0.0001;\n",
    "    return mergedData;\n",
    "#partitioning the data into the traing,validation and test sets    \n",
    "def partitionData(rawData):\n",
    "    trainingLen=math.ceil(rawData.shape[0]*TRAINING_DATA_PERCENT);\n",
    "    testingLen=math.ceil(rawData.shape[0]*TEST_DATA_PERCENT);\n",
    "    validationLen=math.ceil(rawData.shape[0]*VALIDATION_DATA_PERCENT);\n",
    "    return rawData[0:trainingLen,:],rawData[trainingLen:validationLen+trainingLen,:],rawData[validationLen+trainingLen:validationLen+trainingLen+testingLen];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning the variance vector, which has a variance across each and every feature column \n",
    "def generateVarianceVector(trainingFeatureData):\n",
    "    varVect=[];\n",
    "    vect=[];\n",
    "    columns=trainingFeatureData.shape[1];\n",
    "    rows=trainingFeatureData.shape[0];\n",
    "    for k in range(0,trainingFeatureData.shape[1]):\n",
    "        vect=trainingFeatureData[:,k:k+1]    \n",
    "        varVect.append(np.var(vect));\n",
    "    return varVect;\n",
    "#returning the covariance matrix for the training data\n",
    "def generatingCoVarianceMatrix(varianceVector,length):\n",
    "    #covarianceMatrix=[[0 for i in range(0,length)] for j in range(0,length)];\n",
    "    covarianceMatrix=np.zeros((length,length))\n",
    "    for i in range(0,length):\n",
    "        covarianceMatrix[i][i]=varianceVector[i];\n",
    "    return np.dot(200,covarianceMatrix);#multiplying by 200, to broaden the deviation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the value of the gaussian basis fucntion\n",
    "def getScalar(inputDiff,bigSigmaInverse):\n",
    "    halfProduct=np.matmul(inputDiff,bigSigmaInverse);\n",
    "    fullproduct=np.matmul(halfProduct,inputDiff.transpose());\n",
    "    return math.exp(-1*(fullproduct/2));\n",
    "#generating the design matrix for the data, using gaussian radial fucntion as the basis\n",
    "def generateDesignMatrix(bigSigmaInverse,centroidList,mergedFeatureData):\n",
    "    designMatrix = np.zeros((mergedFeatureData.shape[0],centroidList.shape[0]))\n",
    "    for i in range(0,mergedFeatureData.shape[0]):\n",
    "        inputSample=mergedFeatureData[i];\n",
    "        for j in range(0,centroidList.shape[0]):\n",
    "                inputDiff=np.subtract(inputSample,centroidList[j].transpose());\n",
    "                product=getScalar(inputDiff,bigSigmaInverse);\n",
    "                designMatrix[i][j]=product;\n",
    "    return designMatrix;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for computing the outputs, based on the predicted weights\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    return Y\n",
    "\n",
    "#computing the loss for the Erms\n",
    "def getSquaredDifferenceError(VAL_TEST_OUT,valDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((valDataAct[i].item() - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == valDataAct[i]):\n",
    "                counter+=1;\n",
    "    return sum,counter;\n",
    "#calculating Erms\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    error,accuracy=getSquaredDifferenceError(VAL_TEST_OUT,ValDataAct);\n",
    "    accuracyPercent=(accuracy/len(VAL_TEST_OUT) * 100);\n",
    "    return math.sqrt(error/len(VAL_TEST_OUT));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient Descent Solution-:\n",
    "NUMBER_OF_TRANSACTIONS=400;\n",
    "La=2;\n",
    "learningRate=0.01;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    #regularizing the data, in order to avoid the problem of overfitting\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    \n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    print(INTER.shape);\n",
    "    print(T.shape)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "def applyGradientDescent(trainingTargets,validationTargets,testTargets,designMatrixTraining,designMatrixValidation,designMatrixTesting):\n",
    "    #W=GetWeightsClosedForm(designMatrixTraining,trainingTargets,0.01);\n",
    "    W_LIST= [0 for i in range(0,NUMBER_OF_CLUSTERS)]\n",
    "    W=np.asarray(W_LIST);\n",
    "    #W= np.dot(220, W)\n",
    "    L_Erms_TR=[];\n",
    "    L_Erms_Val=[];\n",
    "    L_Erms_Test=[];\n",
    "    for i in range(0,NUMBER_OF_TRANSACTIONS):\n",
    "            Delta_E_D  = -np.dot((trainingTargets[i].item() - np.dot(W.transpose(),designMatrixTraining[i])),designMatrixTraining[i]);\n",
    "            La_Delta_E_W  = np.dot(La,W);\n",
    "            Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "            Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "            #updating the value of the weights\n",
    "            W_T_Next      = W + Delta_W\n",
    "            #assigning the weights to the new wieght vector\n",
    "            W         = W_T_Next\n",
    "            #-----------------TrainingData Accuracy---------------------#\n",
    "            TR_TEST_OUT   = GetValTest(designMatrixTraining,W_T_Next) \n",
    "            Erms_TR       = GetErms(TR_TEST_OUT,trainingTargets)\n",
    "            L_Erms_TR.append(float(Erms_TR))\n",
    "\n",
    "            #-----------------ValidationData Accuracy---------------------#\n",
    "            VAL_TEST_OUT  = GetValTest(designMatrixValidation,W_T_Next) \n",
    "            Erms_Val      = GetErms(VAL_TEST_OUT,validationTargets)\n",
    "            L_Erms_Val.append(float(Erms_Val))\n",
    "\n",
    "            #-----------------TestingData Accuracy---------------------#\n",
    "            TEST_OUT= GetValTest(designMatrixTesting,W_T_Next) \n",
    "            Erms_Test = GetErms(TEST_OUT,testTargets)\n",
    "            L_Erms_Test.append(float(Erms_Test))\n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(weights,targets,trainingData):\n",
    "    h=calculateSigmoid(weights,trainingData);\n",
    "    y=targets\n",
    "    loss=np.multiply(-1 * y,np.log(h))-np.multiply((1-y),np.log(1-h));\n",
    "    cost=np.mean(loss)\n",
    "    erms=getErmsForLogistic(cost,y.shape[0]);\n",
    "\n",
    "def getErmsForLogistic(loss,dataSize):\n",
    "    return np.sqrt(loss*2/dataSize);   \n",
    "#computing the accuracy for logistic regression\n",
    "def computeAccuracy(weights,data,targets):\n",
    "    _Y=calculateSigmoid(data,weights);\n",
    "    counter=0;\n",
    "    _Y=np.around(_Y);\n",
    "    for i in range(0,_Y.shape[0]):\n",
    "        if targets[i].item()==_Y[i].item():\n",
    "            counter=counter+1;\n",
    "    return (counter/targets.shape[0]) * 100;     \n",
    "#applying gradient desecnt, to compute the weights for training data\n",
    "def logisticRegression(trainingData,targets):\n",
    "    W_LIST= np.zeros([trainingData.shape[1], 1])\n",
    "    W=np.asarray(W_LIST);\n",
    "    print(W.shape)\n",
    "    for i in range(0,400):\n",
    "        h=calculateSigmoid(trainingData, W)\n",
    "        X=trainingData.T\n",
    "        delW=np.dot(X,(h-targets))/targets.shape[0]\n",
    "        W-=.01*delW;\n",
    "    return W;\n",
    "        \n",
    "def findSigmoid(z):  \n",
    "    return 1/(1+np.exp(-z));\n",
    "#computing the sigmoid for given weights and data(establishing the genesis equation)\n",
    "def calculateSigmoid(data,weights):\n",
    "    dproduct=np.dot(data, weights);\n",
    "    y=findSigmoid(dproduct);\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriveData(data,featureLength):\n",
    "    featureData=data[:,0:featureLength]\n",
    "    targets=data[:,featureLength:featureLength+1]\n",
    "    return featureData,targets\n",
    "#clustering data using k means\n",
    "def clusterData(trainingFeatureData):\n",
    "    kmeans = KMeans(n_clusters=NUMBER_OF_CLUSTERS, random_state=0).fit(trainingFeatureData)\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def projectKnob(num):\n",
    "    if num==1:# case for human data set(subtraction)\n",
    "        differentWriterDataSet=prepareDataByScheme(diffPairsHumanDataList,imageFeatureMap,False);\n",
    "        sameWriterDataSet=prepareDataByScheme(samePairsHumanDataList,imageFeatureMap,False);\n",
    "    elif num==2:# case for human data set(concatenation)\n",
    "        differentWriterDataSet=prepareDataByScheme(diffPairsHumanDataList,imageFeatureMap,True);\n",
    "        sameWriterDataSet=prepareDataByScheme(samePairsHumanDataList,imageFeatureMap,True);\n",
    "    elif num==3:# case for gsc data set(subtraction) \n",
    "        differentWriterDataSet=prepareDataByScheme(diffPairsGSCDataList,imageFeatureMapGSC,False);\n",
    "        sameWriterDataSet=prepareDataByScheme(samePairsGSCDataList,imageFeatureMapGSC,False)\n",
    "    else:# case for gsc data set(concatenation)\n",
    "        differentWriterDataSet=prepareDataByScheme(diffPairsGSCDataList,imageFeatureMapGSC,True);\n",
    "        sameWriterDataSet=prepareDataByScheme(samePairsGSCDataList,imageFeatureMapGSC,True)\n",
    "    #merging data(same writer data with the different writer data )\n",
    "    mergedData=mergeData(sameWriterDataSet,differentWriterDataSet);\n",
    "    print('--------merge data dim-------',mergedData.shape);\n",
    "    #validating for zero columns(as they can make the covariance matrix singular)\n",
    "    zeroValueColumns=verifyingAllZeroColumns(mergedData);\n",
    "    #removing the zero columns\n",
    "    #mergedData=modifyingMergedData(mergedData,zeroValueColumns);\n",
    "    #shuffling or randomizing the merged data\n",
    "    np.random.shuffle(mergedData);\n",
    "    print('--------random merge data dim-------',mergedData.shape);\n",
    "    #partitioning the data into 3 sets(training,validationData,testData)\n",
    "    trainingData,validationData,testData=partitionData(mergedData);\n",
    "    print('--------training data dim-------',trainingData.shape);\n",
    "    print('--------validation data dim-------',validationData.shape);\n",
    "    print('--------testData data dim-------',testData.shape);\n",
    "    \n",
    "    featureLength=mergedData.shape[1]-1;\n",
    "    #deriving features data, targets data for training set.\n",
    "    trainingFeatureData,trainingTargets=deriveData(trainingData,featureLength);\n",
    "    print('--------training feature data dim-------',trainingFeatureData.shape);\n",
    "    print('--------training targets dim-------',trainingTargets.shape);   \n",
    "   #deriving features data, targets data for validation set.\n",
    "    validationFeatureData,validationTargets=deriveData(validationData,featureLength);\n",
    "    print('--------training feature data dim-------',validationFeatureData.shape);\n",
    "    print('--------training targets dim-------',validationTargets.shape);\n",
    "    #deriving features data, targets data for testing set.    \n",
    "    testFeatureData,testTargets=deriveData(testData,featureLength);\n",
    "    print('--------training feature data dim-------',testFeatureData.shape);\n",
    "    print('--------training targets dim-------',testTargets.shape);   \n",
    "    #applying the logistic regression\n",
    "    logisticPredictedWeights=logisticRegression(trainingFeatureData,trainingTargets);\n",
    "    #computing accuracy using the predicted weights for training,validation and test.\n",
    "    logisticTrainingAccuracy=computeAccuracy(logisticPredictedWeights,trainingFeatureData,trainingTargets);\n",
    "    logisticValidationAccuracy=computeAccuracy(logisticPredictedWeights,validationFeatureData,validationTargets);\n",
    "    logisticTestingAccuracy=computeAccuracy(logisticPredictedWeights,testFeatureData,testTargets)\n",
    "    \n",
    "    print('Training Accuracy: ',logisticTrainingAccuracy)\n",
    "    print('Validation Accuracy: ',logisticValidationAccuracy);\n",
    "    print('Test Accuracy: ',logisticTestingAccuracy);\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #neural network\n",
    "    model=initializeModel(trainingFeatureData);\n",
    "    history=oprateNeuralNetwork(model,trainingFeatureData,validationFeatureData,trainingTargets,validationTargets); \n",
    "    #printKerasGraphs(history);\n",
    "    metric=evaluateNeuralNetwork(model,testFeatureData,testTargets)\n",
    "    print('----neural network accuracy-------',metric *100)\n",
    "    \n",
    "    \n",
    "    #compute the centroid list by kmeans\n",
    "    centroidList=clusterData(trainingFeatureData);   \n",
    "    varianceVector=generateVarianceVector(trainingFeatureData);\n",
    "    #getting big sigma matrix\n",
    "    bigSigma=generatingCoVarianceMatrix(varianceVector,trainingFeatureData.shape[1]);\n",
    "    print('--------big sigma----------',bigSigma.shape)\n",
    "    modifyingMergedData(bigSigma,[]);\n",
    "    #print(bigSigma);\n",
    "    #computing the inverse of the bigsigma\n",
    "    bigSigmaInverse=np.linalg.inv(bigSigma);\n",
    "    #creating design matrix for training\n",
    "    designMatrixTraining=generateDesignMatrix(bigSigmaInverse,centroidList,trainingFeatureData);\n",
    "    print('--------training design matrix dim-------',designMatrixTraining.shape);  \n",
    "    #creating design matrix for validation\n",
    "    designMatrixValidation=generateDesignMatrix(bigSigmaInverse,centroidList,validationFeatureData);\n",
    "    print('--------validation design matrix dim-------',designMatrixValidation.shape);\n",
    "    #creating design matrix for testing\n",
    "    designMatrixTesting=generateDesignMatrix(bigSigmaInverse,centroidList,testFeatureData);\n",
    "    print('--------testing design matrix dim-------',designMatrixTesting.shape);\n",
    "    #applying gradient descent for the linear regression\n",
    "    predictedWeights=applyGradientDescent(trainingTargets,validationTargets,testTargets,designMatrixTraining,designMatrixValidation,designMatrixTesting);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------merge data dim------- (20000, 513)\n",
      "--------random merge data dim------- (20000, 513)\n",
      "--------training data dim------- (16000, 513)\n",
      "--------validation data dim------- (2000, 513)\n",
      "--------testData data dim------- (2000, 513)\n",
      "--------training feature data dim------- (16000, 512)\n",
      "--------training targets dim------- (16000, 1)\n",
      "--------training feature data dim------- (2000, 512)\n",
      "--------training targets dim------- (2000, 1)\n",
      "--------training feature data dim------- (2000, 512)\n",
      "--------training targets dim------- (2000, 1)\n",
      "(512, 1)\n",
      "Training Accuracy:  57.0125\n",
      "Validation Accuracy:  56.05\n",
      "Test Accuracy:  53.05\n",
      "Train on 15300 samples, validate on 2700 samples\n",
      "Epoch 1/25\n",
      "15300/15300 [==============================] - 11s 713us/step - loss: 0.6057 - acc: 0.6546 - val_loss: 0.4812 - val_acc: 0.7752\n",
      "Epoch 2/25\n",
      "15300/15300 [==============================] - 8s 540us/step - loss: 0.4093 - acc: 0.8190 - val_loss: 0.4016 - val_acc: 0.8215\n",
      "Epoch 3/25\n",
      "15300/15300 [==============================] - 8s 522us/step - loss: 0.2904 - acc: 0.8822 - val_loss: 0.3651 - val_acc: 0.8474\n",
      "Epoch 4/25\n",
      "15300/15300 [==============================] - 8s 521us/step - loss: 0.1941 - acc: 0.9263 - val_loss: 0.3706 - val_acc: 0.8578\n",
      "Epoch 5/25\n",
      "15300/15300 [==============================] - 9s 561us/step - loss: 0.1272 - acc: 0.9565 - val_loss: 0.3672 - val_acc: 0.8667\n",
      "Epoch 6/25\n",
      "15300/15300 [==============================] - 9s 573us/step - loss: 0.0747 - acc: 0.9756 - val_loss: 0.3981 - val_acc: 0.8711\n",
      "Epoch 7/25\n",
      "15300/15300 [==============================] - 8s 554us/step - loss: 0.0426 - acc: 0.9878 - val_loss: 0.4379 - val_acc: 0.8789\n",
      "Epoch 8/25\n",
      "15300/15300 [==============================] - 9s 558us/step - loss: 0.0252 - acc: 0.9941 - val_loss: 0.4762 - val_acc: 0.8781\n",
      "Epoch 9/25\n",
      "15300/15300 [==============================] - 8s 523us/step - loss: 0.0123 - acc: 0.9980 - val_loss: 0.4794 - val_acc: 0.8885\n",
      "Epoch 10/25\n",
      "15300/15300 [==============================] - 9s 557us/step - loss: 0.0049 - acc: 0.9997 - val_loss: 0.4849 - val_acc: 0.8926\n",
      "Epoch 11/25\n",
      "15300/15300 [==============================] - 8s 556us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4971 - val_acc: 0.8919\n",
      "Epoch 12/25\n",
      "15300/15300 [==============================] - 9s 586us/step - loss: 9.7669e-04 - acc: 1.0000 - val_loss: 0.5063 - val_acc: 0.8922\n",
      "Epoch 13/25\n",
      "15300/15300 [==============================] - 8s 536us/step - loss: 7.4391e-04 - acc: 1.0000 - val_loss: 0.5166 - val_acc: 0.8922\n",
      "Epoch 14/25\n",
      "15300/15300 [==============================] - 8s 532us/step - loss: 6.0045e-04 - acc: 1.0000 - val_loss: 0.5263 - val_acc: 0.8919\n",
      "Epoch 15/25\n",
      "15300/15300 [==============================] - 9s 606us/step - loss: 4.9765e-04 - acc: 1.0000 - val_loss: 0.5331 - val_acc: 0.8919\n",
      "Epoch 16/25\n",
      "15300/15300 [==============================] - 9s 569us/step - loss: 4.1910e-04 - acc: 1.0000 - val_loss: 0.5418 - val_acc: 0.8915\n",
      "Epoch 17/25\n",
      "15300/15300 [==============================] - 8s 523us/step - loss: 3.5684e-04 - acc: 1.0000 - val_loss: 0.5499 - val_acc: 0.8915\n",
      "Epoch 18/25\n",
      "15300/15300 [==============================] - 8s 513us/step - loss: 3.0889e-04 - acc: 1.0000 - val_loss: 0.5558 - val_acc: 0.8911\n",
      "Epoch 19/25\n",
      "15300/15300 [==============================] - 8s 511us/step - loss: 2.6752e-04 - acc: 1.0000 - val_loss: 0.5628 - val_acc: 0.8911\n",
      "Epoch 20/25\n",
      "15300/15300 [==============================] - 8s 510us/step - loss: 2.3417e-04 - acc: 1.0000 - val_loss: 0.5688 - val_acc: 0.8911\n",
      "Epoch 21/25\n",
      "15300/15300 [==============================] - 8s 508us/step - loss: 2.0584e-04 - acc: 1.0000 - val_loss: 0.5758 - val_acc: 0.8907\n",
      "Epoch 22/25\n",
      "15300/15300 [==============================] - 8s 518us/step - loss: 1.8232e-04 - acc: 1.0000 - val_loss: 0.5804 - val_acc: 0.8911\n",
      "Epoch 23/25\n",
      "15300/15300 [==============================] - 8s 537us/step - loss: 1.6192e-04 - acc: 1.0000 - val_loss: 0.5863 - val_acc: 0.8911\n",
      "Epoch 24/25\n",
      "15300/15300 [==============================] - 8s 517us/step - loss: 1.4479e-04 - acc: 1.0000 - val_loss: 0.5909 - val_acc: 0.8915\n",
      "Epoch 25/25\n",
      "15300/15300 [==============================] - 8s 521us/step - loss: 1.2933e-04 - acc: 1.0000 - val_loss: 0.5971 - val_acc: 0.8907\n",
      "2000/2000 [==============================] - 0s 244us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-da63ca38150e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprojectKnob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-5e81f5c843e6>\u001b[0m in \u001b[0;36mprojectKnob\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moprateNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainingFeatureData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationFeatureData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainingTargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#printKerasGraphs(history);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mneuralLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluateNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestFeatureData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----neural network accuracy-------'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in range(1,5,1):\n",
    "    projectKnob(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee=hh[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTargets[2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merf=np.delete(mergedData,zeroValueColumns,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.pow(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix().transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(18).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=np.asarray(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=ss.transpose();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
